{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    " roc_auc_score, roc_curve, classification_report, confusion_matrix, auc)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration and Setup\n",
    "\n",
    "# Parameters\n",
    "needle_height = '1.3'\n",
    "conjugate = 'chlr'\n",
    "n_trials = 50\n",
    "dataset_key = f\"{needle_height}_{conjugate}\"\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"results/{dataset_key}_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Base directory\n",
    "base_dir = r\"D:\\20241129_solid_nN_1.3_2.4_mdck_siRNA_tnsfn_chlr\"\n",
    "dataset_path = base_dir + r\"solid_1.3_chlr_cell_level.csv\"\n",
    "\n",
    "# Define morphological and intensity features\n",
    "cell_morph_features = [\n",
    " 'area', 'perimeter', 'major_axis_length', 'minor_axis_length', \n",
    " 'eccentricity', 'circularity', 'solidity', 'orientation'\n",
    "]\n",
    "\n",
    "nuclear_morph_features = [\n",
    " 'nuclear_area', 'nuclear_perimeter', 'nuclear_major_axis_length', \n",
    " 'nuclear_minor_axis_length', 'nuclear_eccentricity', 'nuclear_circularity', \n",
    " 'nuclear_solidity', 'nuclear_orientation'\n",
    "]\n",
    "\n",
    "channel_feature_suffixes = [\n",
    " 'intensity_p10', 'intensity_p25', 'intensity_p50', \n",
    " 'intensity_p75', 'intensity_p90'\n",
    "]\n",
    "\n",
    "protein_channels = ['actin', 'caveolin', 'clathrin_hc', 'nuclei']\n",
    "\n",
    "# Generate feature list with caveolin features first to ensure dominance\n",
    "feature_list = cell_morph_features + nuclear_morph_features\n",
    "\n",
    "for suffix in channel_feature_suffixes:\n",
    " feature_list.append(f\"caveolin_{suffix}\")\n",
    "\n",
    "for ch in protein_channels:\n",
    " if ch != 'caveolin':\n",
    "  for suffix in channel_feature_suffixes:\n",
    "   feature_list.append(f\"{ch}_{suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Training and Evaluation\n",
    "\n",
    "def process_dataset(dataset_path, dataset_name, area_percentiles=(2, 98)):\n",
    " print(f\"\\n=== Processing {dataset_name} ===\")\n",
    " \n",
    " # Extract conjugate type from dataset_name\n",
    " conjugate_type = dataset_name.split('_')[1]\n",
    " \n",
    " # Set the correct intensity column name\n",
    " intensity_column = f\"{conjugate_type}_intensity_mean\"\n",
    " \n",
    " print(f\"Using intensity column: {intensity_column}\")\n",
    " \n",
    " # Load dataset\n",
    " df = pd.read_csv(dataset_path)\n",
    " \n",
    " # Threshold for chlr\n",
    " intensity_threshold = 300\n",
    " \n",
    " # Apply area filtering based on percentiles (2, 98)\n",
    " cell_area_min, cell_area_max = np.percentile(df['area'], area_percentiles)\n",
    " nuclear_area_min, nuclear_area_max = np.percentile(df['nuclear_area'], area_percentiles)\n",
    " \n",
    " # Filter cells and nuclei based on thresholds\n",
    " df_filtered = df[\n",
    "  (df['area'] >= cell_area_min) & \n",
    "  (df['area'] <= cell_area_max) & \n",
    "  (df[intensity_column] > intensity_threshold)\n",
    " ].copy()\n",
    " \n",
    " nuclei_threshold = (\n",
    "  (df_filtered['nuclear_area'] >= nuclear_area_min) & \n",
    "  (df_filtered['nuclear_area'] <= nuclear_area_max)\n",
    " )\n",
    " \n",
    " nuclear_cols = [col for col in df_filtered.columns if col.startswith('nuclear_')]\n",
    " df_filtered.loc[~nuclei_threshold, nuclear_cols] = np.nan\n",
    " \n",
    " # Binarise target variable\n",
    " df_filtered['conjugate_category'] = np.where(\n",
    "  df_filtered[intensity_column] > 300,\n",
    "  1,\n",
    "  0\n",
    " )\n",
    " \n",
    " print(\"Unique conjugate_category values:\", df_filtered['conjugate_category'].unique())\n",
    " print(\"Value counts:\\n\", df_filtered['conjugate_category'].value_counts())\n",
    " \n",
    " y = df_filtered['conjugate_category']\n",
    " \n",
    " label_encoder = LabelEncoder()\n",
    " y_encoded = label_encoder.fit_transform(y)\n",
    " \n",
    " X = df_filtered[feature_list]\n",
    " \n",
    " images = df_filtered['image_id']\n",
    " \n",
    " # Dictionaries for aggregated metrics\n",
    " all_fold_metrics = []\n",
    " class_report_list = []\n",
    " shap_values_list = []\n",
    " mean_fpr = np.linspace(0, 1, 100)\n",
    " tprs = []\n",
    " aucs = []\n",
    " \n",
    " # Class distribution tracking\n",
    " class_distributions = []\n",
    " \n",
    " # Create dictionaries to store performance metrics\n",
    " train_metrics = {}\n",
    " test_metrics = {}\n",
    " aggregated_metrics = {\n",
    "  'accuracy': [],\n",
    "  'precision': [],\n",
    "  'recall': [],\n",
    "  'f1': [],\n",
    "  'roc_auc': []\n",
    " }\n",
    " \n",
    " # Outer CV: Stratified Group K-Fold\n",
    " outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    " \n",
    " # Dummy model for SHAP initialisation\n",
    " dummy_model = xgb.XGBClassifier()\n",
    " dummy_model.fit(X.iloc[:10], y_encoded[:10])\n",
    " explainer = shap.TreeExplainer(dummy_model)\n",
    " \n",
    " for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y_encoded, groups=images), start=1):\n",
    "  print(f\"\\n=== Outer Fold {fold} ===\")\n",
    "  print(f\"Fold {fold}: n_test={len(test_idx)}\")\n",
    "  print(f\"Class distribution: Class 0: {np.sum(y_encoded[test_idx] == 0)}, Class 1: {np.sum(y_encoded[test_idx] == 1)}\")\n",
    "  \n",
    "  X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "  y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "  \n",
    "  # Class distribution tracking in fold\n",
    "  class_distributions.append({\n",
    "   \"train\": np.bincount(y_train, minlength=2),\n",
    "   \"test\": np.bincount(y_test, minlength=2)\n",
    "  })\n",
    "  \n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "  \n",
    "  def objective(trial):\n",
    "   params = {\n",
    "    'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "    'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 50, 200)\n",
    "   }\n",
    "   \n",
    "   model = xgb.XGBClassifier(random_state=42, **params)\n",
    "   \n",
    "   inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "   inner_scores = []\n",
    "   \n",
    "   for inner_train_idx, inner_valid_idx in inner_cv.split(X_train_scaled, y_train):\n",
    "    X_inner_train = X_train_scaled[inner_train_idx]\n",
    "    X_inner_valid = X_train_scaled[inner_valid_idx]\n",
    "    y_inner_train = y_train[inner_train_idx]\n",
    "    y_inner_valid = y_train[inner_valid_idx]\n",
    "    \n",
    "    model.fit(X_inner_train, y_inner_train)\n",
    "    y_pred_inner = model.predict(X_inner_valid)\n",
    "    score = accuracy_score(y_inner_valid, y_pred_inner)\n",
    "    inner_scores.append(score)\n",
    "   \n",
    "   return np.mean(inner_scores)\n",
    "  \n",
    "  study = optuna.create_study(direction='maximize')\n",
    "  study.optimize(objective, n_trials=n_trials)\n",
    "  \n",
    "  best_params = study.best_params\n",
    "  \n",
    "  best_model = xgb.XGBClassifier(random_state=42, **best_params)\n",
    "  best_model.fit(X_train_scaled, y_train)\n",
    "  \n",
    "  # Calculate comprehensive metrics\n",
    "  y_test_pred = best_model.predict(X_test_scaled)\n",
    "  y_test_proba = best_model.predict_proba(X_test_scaled)\n",
    "  \n",
    "  # Store fold metrics\n",
    "  fold_metrics = {\n",
    "   \"fold\": fold,\n",
    "   \"accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "   \"f1_weighted\": f1_score(y_test, y_test_pred, average='weighted'),\n",
    "   \"precision_weighted\": precision_score(y_test, y_test_pred, average='weighted'),\n",
    "   \"recall_weighted\": recall_score(y_test, y_test_pred, average='weighted'),\n",
    "   \"roc_auc\": roc_auc_score(y_test, y_test_proba, multi_class='ovr')\n",
    "  }\n",
    "  all_fold_metrics.append(fold_metrics)\n",
    "  \n",
    "  # Generate class-wise metrics\n",
    "  class_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "  class_report_list.append(class_report)\n",
    "  \n",
    "  # Calculate SHAP values\n",
    "  explainer = shap.TreeExplainer(best_model)\n",
    "  shap_values = explainer.shap_values(X_test_scaled)\n",
    "  shap_values_list.append(shap_values)\n",
    "  \n",
    "  # Save model\n",
    "  model_filename = f\"{output_dir}/model_{dataset_name}_fold_{fold}.joblib\"\n",
    "  joblib.dump(best_model, model_filename)\n",
    "  print(f\"Model saved as {model_filename}\")\n",
    "  \n",
    "  try:\n",
    "   fpr, tpr, _ = roc_curve((y_test == 1).astype(int), y_test_proba[:, 1])\n",
    "   roc_auc = auc(fpr, tpr)\n",
    "   \n",
    "   interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "   interp_tpr[0] = 0.0\n",
    "   tprs.append(interp_tpr)\n",
    "   aucs.append(roc_auc)\n",
    "  except Exception as e:\n",
    "   print(f\"Error calculating ROC: {str(e)}\")\n",
    " \n",
    " # After all folds complete\n",
    " metrics_df = pd.DataFrame(all_fold_metrics)\n",
    " avg_metrics = {\n",
    "  'accuracy': metrics_df['accuracy'].mean(),\n",
    "  'accuracy_std': metrics_df['accuracy'].std(),\n",
    "  'f1_weighted': metrics_df['f1_weighted'].mean(),\n",
    "  'f1_weighted_std': metrics_df['f1_weighted'].std(),\n",
    "  'precision_weighted': metrics_df['precision_weighted'].mean(),\n",
    "  'precision_weighted_std': metrics_df['precision_weighted'].std(),\n",
    "  'recall_weighted': metrics_df['recall_weighted'].mean(),\n",
    "  'recall_weighted_std': metrics_df['recall_weighted'].std(),\n",
    "  'roc_auc': metrics_df['roc_auc'].mean(),\n",
    "  'roc_auc_std': metrics_df['roc_auc'].std()\n",
    " }\n",
    " \n",
    " best_fold_idx = np.argmax(metrics_df['roc_auc'])\n",
    " best_fold = metrics_df.iloc[best_fold_idx]['fold']\n",
    " best_model_path = f\"{output_dir}/model_{dataset_name}_fold_{best_fold}.joblib\"\n",
    " best_model = joblib.load(best_model_path)\n",
    " \n",
    " return avg_metrics, best_model, best_fold, tprs, aucs, mean_fpr, shap_values_list, X, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualisation and Analysis\n",
    "\n",
    "def create_visualizations(avg_metrics, best_model, best_fold, tprs, aucs, mean_fpr, shap_values_list, X, test_idx, dataset_name, output_dir):\n",
    " # Aggregate ROC curve\n",
    " if len(tprs) > 0 and len(aucs) > 0:\n",
    "  try:\n",
    "   fig, ax = plt.subplots(figsize=(10, 8))\n",
    "   \n",
    "   for i, tpr in enumerate(tprs):\n",
    "    ax.plot(mean_fpr, tpr, alpha=0.3, label=f'ROC fold {i+1} (AUC = {aucs[i]:.2f})')\n",
    "   \n",
    "   mean_tpr = np.mean(tprs, axis=0)\n",
    "   mean_auc = auc(mean_fpr, mean_tpr)\n",
    "   std_auc = np.std(aucs)\n",
    "   ax.plot(mean_fpr, mean_tpr, 'b-', label=f'Mean ROC (AUC = {mean_auc:.2f} ± {std_auc:.2f})', lw=2)\n",
    "   \n",
    "   std_tpr = np.std(tprs, axis=0)\n",
    "   tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "   tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "   ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.2, label=r'± 1 std. dev.')\n",
    "   \n",
    "   ax.plot([0, 1], [0, 1], 'k--')\n",
    "   ax.set_xlim([0.0, 1.0])\n",
    "   ax.set_ylim([0.0, 1.05])\n",
    "   ax.set_xlabel('False Positive Rate')\n",
    "   ax.set_ylabel('True Positive Rate')\n",
    "   ax.set_title('Aggregate ROC Curve for Class 1')\n",
    "   ax.legend(loc=\"lower right\")\n",
    "   plt.savefig(f\"{output_dir}/aggregate_roc_class1_{dataset_name}.png\")\n",
    "   plt.close(fig)\n",
    "  except Exception as e:\n",
    "   print(f\"Error generating aggregate ROC curve: {str(e)}\")\n",
    " \n",
    " # Best fold ROC curve\n",
    " try:\n",
    "  fig, ax = plt.subplots(figsize=(10, 8))\n",
    "  X_test = X.iloc[test_idx]\n",
    "  y_test = y_encoded[test_idx]\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "  y_test_proba = best_model.predict_proba(X_test_scaled)\n",
    "  \n",
    "  fpr, tpr, _ = roc_curve((y_test == 1).astype(int), y_test_proba[:, 1])\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  ax.plot(fpr, tpr, label=f'Class 1 (AUC = {roc_auc:.2f})')\n",
    "  ax.plot([0, 1], [0, 1], 'k--')\n",
    "  ax.set_xlabel('False Positive Rate')\n",
    "  ax.set_ylabel('True Positive Rate')\n",
    "  ax.set_title(f'ROC Curve for Class 1 - Best Fold {best_fold}')\n",
    "  ax.legend()\n",
    "  plt.savefig(f\"{output_dir}/best_fold_roc_class1_{dataset_name}.png\")\n",
    "  plt.close(fig)\n",
    " except Exception as e:\n",
    "  print(f\"Error generating best fold ROC curve: {str(e)}\")\n",
    " \n",
    " # Aggregate SHAP plot\n",
    " try:\n",
    "  fig, ax = plt.subplots(figsize=(12, 10))\n",
    "  shap_values_combined = np.vstack([sv for sv in shap_values_list])\n",
    "  X_test_combined = pd.concat([X.iloc[test_idx] for _, test_idx in list(outer_cv.split(X, y_encoded, groups=images))])\n",
    "  shap.summary_plot(shap_values_combined, X_test_combined, plot_type=\"beeswarm\", show=False)\n",
    "  plt.title('Aggregate SHAP Feature Importance')\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"{output_dir}/aggregate_shap_beeswarm_{dataset_name}.png\")\n",
    "  plt.close(fig)\n",
    " except Exception as e:\n",
    "  print(f\"Error generating aggregate SHAP plot: {str(e)}\")\n",
    " \n",
    " # Best fold SHAP plot\n",
    " try:\n",
    "  fig, ax = plt.subplots(figsize=(12, 10))\n",
    "  X_test = X.iloc[test_idx]\n",
    "  explainer = shap.TreeExplainer(best_model)\n",
    "  shap_values = explainer.shap_values(X_test_scaled)\n",
    "  shap.summary_plot(shap_values, X_test, plot_type=\"beeswarm\", show=False)\n",
    "  plt.title(f'SHAP Feature Importance - Best Fold {best_fold}')\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"{output_dir}/best_fold_shap_beeswarm_{dataset_name}.png\")\n",
    "  plt.close(fig)\n",
    " except Exception as e:\n",
    "  print(f\"Error generating best fold SHAP plot: {str(e)}\")\n",
    " \n",
    " # Print final results\n",
    " print(\"\\n=== Final Results ===\")\n",
    " print(f\"Dataset: {dataset_name}\")\n",
    " print(f\"Accuracy: {avg_metrics['accuracy']:.4f} ± {avg_metrics['accuracy_std']:.4f}\")\n",
    " print(f\"F1 Score (weighted): {avg_metrics['f1_weighted']:.4f} ± {avg_metrics['f1_weighted_std']:.4f}\")\n",
    " print(f\"Precision (weighted): {avg_metrics['precision_weighted']:.4f} ± {avg_metrics['precision_weighted_std']:.4f}\")\n",
    " print(f\"Recall (weighted): {avg_metrics['recall_weighted']:.4f} ± {avg_metrics['recall_weighted_std']:.4f}\")\n",
    " print(f\"ROC AUC: {avg_metrics['roc_auc']:.4f} ± {avg_metrics['roc_auc_std']:.4f}\")\n",
    " print(f\"Best Fold: {best_fold}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
